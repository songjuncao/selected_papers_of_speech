- UALM: Unified Audio Language Model for Understanding, Generation and Reasoning
- ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling
- HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation
- AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation
- AudioX: Diffusion Transformer for Anything-to-Audio Generation
- TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization
- Tri-Ergon: Fine-grained Video-to-Audio Generation with Multi-modal Conditions and LUFS Control
  - AAAI 2025
- Video-Guided Foley Sound Generation with Multimodal Controls
- STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment
- Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis
- Efficient Autoregressive Audio Modeling via Next-Scale Prediction
- Stable Audio Open
  - https://github.com/Stability-AI/stable-audio-tools
- Video-to-Audio Generation with Hidden Alignment
- Read, Watch and Scream! Sound Generation from Text and Video
- Improving Audio Generation with Visual Enhanced Caption
- SpecMaskGIT: Masked Generative Modeling of Audio Spectrograms for Efficient Audio Synthesis and Beyond
- AudioLCM: Text-to-Audio Generation with Latent Consistency Models
- Frieren: Efficient Video-to-Audio Generation with Rectified Flow Matching
- A survey of deep learning audio generation methods
- Audio-Journey: Open Domain Latent Diffusion Based Text-To-Audio Generation
- Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization
- Text-to-Audio Generation Synchronized with Videos
- V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models
- Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners
