- MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning
- From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech
  - CVPR 2025
- Synchronized Video-to-Audio Generation via MelQuantization-Continuum Decomposition
  - CVPR 2025
- MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment
- It Takes Two: Real-time Co-Speech Two-personâ€™s Interaction Generation via Reactive Auto-regressive Diffusion Model
- OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows
- Gotta Hear Them All: Sound Source-Aware Vision to Audio Generation
- LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis
- Movie Gen: A Cast of Media Foundation Models
  - Meta
- From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation
  - ICML 2024
- FastTalker: Jointly Generating Speech and Conversational Gestures from Text
- Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation
- Faces that Speak: Jointly Synthesising Talking Face and Speech from Text
  - CVPR 2024
- Semantically consistent Video-to-Audio Generation using Multimodal Language Large Model
- FoleyGen: Visually-Guided Audio Generation
- V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models
- I Hear Your True Colors: Image Guided Audio Generation
