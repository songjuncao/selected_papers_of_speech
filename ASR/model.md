## MoE
- Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters
## former
- LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units
- SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding
- https://github.com/SamsungLabs/SummaryMixing
