## distill & efficiency
- Massive End-to-end Speech Recognition Models with Time Reduction
  - NAACL 2024
  - Google
- uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation via Large-Scale Pseudo Labelling
## joint
- Jointly Recognizing Speech and Singing Voices Based on Multi-Task Audio Source Separation
  - ICME 2024
## bias
- Deferred NAM: Low-latency Top-K Context Injection via Deferred Context Encoding for Non-Streaming ASR
  - Google
  - NAACL 2024
## adapter
- HIERARCHICAL RECURRENT ADAPTERS FOR EFFICIENT MULTI-TASK ADAPTATION OF LARGE SPEECH MODELS
- A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models
- Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models
## general
- Advanced Long-Content Speech Recognition with Factorized Neural Transducer
  - Microsoft
  - TASLP
- Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models
  - Google
  - ICASSP 2024
## MoE
- U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF
- Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters
## former
- Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study
- Augmenting conformers with structured state-space sequence models for online speech recognition
- LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units
- SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding
- https://github.com/SamsungLabs/SummaryMixing
