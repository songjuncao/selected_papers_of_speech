## adapter
- Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models
## general
- Advanced Long-Content Speech Recognition with Factorized Neural Transducer
  - Microsoft
  - TASLP
- Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models
  - Google
  - ICASSP 2024
## MoE
- Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters
## former
- LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units
- SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding
- https://github.com/SamsungLabs/SummaryMixing
